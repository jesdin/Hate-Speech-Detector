{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom keras.models import Model\nfrom keras.layers import Dense, Embedding, Input\nfrom keras.layers import Conv1D, GlobalMaxPool1D, Dropout, concatenate\nfrom keras.callbacks import Callback, ModelCheckpoint\nfrom keras.preprocessing import text, sequence\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\nfrom IPython.display import clear_output","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load and Preprocessing Steps\nHere we load the data and fill in the misisng values","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv(\"../input/bengali-hate-speech-dataset/Bengali hate speech .csv\")\nlist_sentences_train = train[\"sentence\"].fillna(\"Invalid\").values\nlist_classes = [\"hate\"]\ny = train[list_classes].values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# define network parameters\nmax_features = 20000\nmaxlen = 100","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Sequence Generation\nHere we take the data and generate sequences from the data","metadata":{}},{"cell_type":"code","source":"tokenizer = text.Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(list(list_sentences_train))\n# train data\nlist_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\nX_t = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('tokenizer.pickle', 'wb') as handle:\n    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def build_model(conv_layers = 2, max_dilation_rate = 4):\n    embed_size = 128\n    inp = Input(shape=(maxlen, ))\n    x = Embedding(max_features, embed_size)(inp)\n    x = Dropout(0.15)(x)\n    x = Conv1D(2*embed_size, \n                   kernel_size = 3)(x)\n    prefilt_x = Conv1D(2*embed_size, \n                   kernel_size = 3)(x)\n    out_conv = []\n    # dilation rate lets us use ngrams and skip grams to process \n    for dilation_rate in range(max_dilation_rate):\n        x = prefilt_x\n        for i in range(3):\n            x = Conv1D(32*2**(i), \n                       kernel_size = 3, \n                       dilation_rate = 2**dilation_rate)(x)    \n        out_conv += [Dropout(0.2)(GlobalMaxPool1D()(x))]\n    x = concatenate(out_conv, axis = -1)    \n    x = Dense(64, activation=\"relu\")(x)\n    x = Dropout(0.1)(x)\n    x = Dense(1, activation=\"sigmoid\")(x)\n    model = Model(inputs=inp, outputs=x)\n    model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['binary_accuracy'])\n\n    return model\n\nmodel = build_model()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train the Model\nHere we train the model and use model checkpointing and early stopping to keep only the best version of the model","metadata":{}},{"cell_type":"code","source":"batch_size = 512\nepochs = 15\n\nfile_path=\"weights.hdf5\"\ncheckpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\nearly = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n\ncallbacks_list = [checkpoint, early] #early\nmodel.fit(X_t, y, \n          batch_size=batch_size, \n          epochs=epochs, \n          validation_split=0.1, \n          callbacks=callbacks_list)\nmodel.load_weights(file_path)\n# clear_output()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Show the results as reference\nSince we clear out all the training data","metadata":{}},{"cell_type":"code","source":"eval_results = model.evaluate(X_t, y, batch_size=batch_size)\nfor c_name, c_val in zip(model.metrics_names, eval_results):\n    print(c_name, '%2.3f' % (c_val))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Check Model","metadata":{}},{"cell_type":"code","source":"model.load_weights(\"./weights.hdf5\");","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenized_sen = tokenizer.texts_to_sequences(\"সব শালারা চামচা মুসফিকুর রহিমের। এসব দালালি বাদ দাও কুকুরের বাচ্ছা এসব করে দেশটা নষ্ট করেছো শুধু নেত...\")\nx = sequence.pad_sequences(tokenized_sen, maxlen=maxlen)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"round(model.predict(x).mean())","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}